{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b81fbc0b-16be-4045-863c-50f32e6b6573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Final Q-table:\n",
      "[[[ 3.74287202e+00  3.18415883e+00  3.97341650e+00  4.84585100e+00]\n",
      "  [ 4.72644290e+00  4.68590322e+00  3.93199106e+00  5.49539000e+00]\n",
      "  [ 5.46451459e+00  6.21710000e+00  4.77270767e+00  5.41097649e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-1.90000000e-02 -2.83176525e-02 -2.96200000e-02 -2.88100000e-02]]\n",
      "\n",
      " [[ 2.10404084e+00  4.64865355e+00  1.05204157e-01  3.09332476e-01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 5.39923613e+00  7.01900000e+00  6.09834463e+00  6.15568341e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-3.69100000e-02  3.24316969e+00 -3.61000000e-02 -1.99000000e-02]]\n",
      "\n",
      " [[ 4.36577909e-01  1.41436858e+00  8.11414255e-01  6.00062355e+00]\n",
      "  [ 3.91832800e+00  9.60010065e-01  1.88082150e+00  7.01492129e+00]\n",
      "  [ 6.18027338e+00  7.00026282e+00  6.07711998e+00  7.91000000e+00]\n",
      "  [ 7.86548128e+00  8.90000000e+00  6.98130116e+00  6.70244902e+00]\n",
      "  [ 7.11011210e-01  3.59917931e+00  7.84841755e+00  3.85518688e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 1.81429448e+00  2.28377943e+00  5.92118131e-03 -7.36267353e-02]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 7.79024992e+00  1.00000000e+01  8.85695643e+00  8.87101602e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 3.17122839e-02 -6.12564181e-03  4.66269605e-03  1.10226941e+00]\n",
      "  [ 3.56110732e-01  1.55917060e+00  9.74651449e-02  4.66880425e+00]\n",
      "  [ 1.28622601e+00  1.07945996e+00  4.66205850e-01  8.14697981e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "#================================== Define the maze layout ============================================\n",
    "maze = np.array([\n",
    "    [0, 0, 0, -1, 0],  # 0: free space, -1: obstacle, 1: goal\n",
    "    [0, -1, 0, -1, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [-1, 0, -1, 0, -1],\n",
    "    [0, 0, 0, 1, 0]     # Goal is at (4, 3)\n",
    "])\n",
    "#================================= Define hyperparameters ==============================================\n",
    "alpha = 0.1               # Learning rate\n",
    "gamma = 0.9               # Discount factor\n",
    "epsilon = 0.8             # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 500\n",
    "#=========================================== Define actions ============================================\n",
    "actions = {\n",
    "    0: (-1, 0),  # Up\n",
    "    1: (1, 0),   # Down\n",
    "    2: (0, -1),  # Left\n",
    "    3: (0, 1)    # Right\n",
    "}\n",
    "#============================== Initialize Q-table with zeros (size of the maze x number of actions) ==========================================\n",
    "no_rows, no_cols = maze.shape\n",
    "q_table = np.zeros((no_rows, no_cols, 4))  # 4 possible actions (up, down, left, right)\n",
    "\n",
    "#======================================== Define function to check if a position is valid =========================================\n",
    "def is_valid_position(maze, position):\n",
    "    x, y = position\n",
    "    return 0 <= x < no_rows and 0 <= y < no_cols and maze[x, y] != -1\n",
    "\n",
    "#======================================= Define function to get next position based on action ======================================\n",
    "def get_next_position(position, action):\n",
    "    x, y = position\n",
    "    dx, dy = actions[action]\n",
    "    next_position = (x + dx, y + dy)\n",
    "    return next_position if is_valid_position(maze, next_position) else position\n",
    "#===================================================== Define reward function ====================================================\n",
    "def get_reward(position):\n",
    "    if maze[position] == 1:\n",
    "        return 10  # Goal reward\n",
    "    elif maze[position] == -1:\n",
    "        return -1  # Penalty for hitting an obstacle\n",
    "    else:\n",
    "        return -0.1  # Small penalty for each step taken\n",
    "#====================================================== Q-learning algorithm ===================================================\n",
    "for episode in range(num_episodes):\n",
    "    position = (0, 0)  # Start position at top-left corner\n",
    "    done = False\n",
    "    while not done:\n",
    "#==================== Choose action based on epsilon-greedy policy =============================\n",
    "if random.uniform(0, 1) < epsilon:\n",
    "    action = random.randint(0, 3)  # Explore: random action\n",
    "else:\n",
    "    action = np.argmax(q_table[position[0], position[1]])  # Exploit: best action\n",
    "#======================================= Take action and observe next state and reward =====================================\n",
    "next_position = get_next_position(position, action)\n",
    "reward = get_reward(next_position)\n",
    "#======================================== Update Q-value using Q-learning formula =========================================\n",
    "q_table[position[0], position[1], action] += alpha * (reward + gamma * np.max(q_table[next_position[0], next_position[1]]) - q_table[position[0], position[1], action])\n",
    "\n",
    "#============================================== Update position ================================================\n",
    "position = next_position\n",
    "#=========================================== Check if goal is reached =============================================\n",
    "if maze[position] == 1:\n",
    "    done = True\n",
    "#======================================================= Decay epsilon to reduce exploration over time =================================\n",
    "epsilon *= epsilon_decay\n",
    "#======================================================= Display Final Q-table =================================================\n",
    "print(\"Training completed!\")\n",
    "print(\"Final Q-table:\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7f312-66da-4912-ba48-2c224665cf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
